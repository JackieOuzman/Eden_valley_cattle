---
title: "Eden Valley Cattle VF"
author: "Jackie Ouzman"
date: "18 July 2019"
output: html_document
---

```{r setup install packages, include=FALSE}
library(tidyverse)
library(dplyr)
library(lubridate)
library(readxl)

library(ggplot2)
library(readr)
library(sp)
library(biogeo)
library(stringr)
library(rgdal)
library(sf)

library(gganimate)
library(png)
library(gifski)

knitr::opts_chunk$set(include = FALSE)
```

## The first steps is to bring in the data files

After the files are downloaded from the collar they get 'converted'
Jim Lea from Armidale does this.
The files get a folder with a date and within this is a series of csv files.
One file for each collar each day.

At the monmnet the data (for the older collars are located here:

W:/VF/Eden_Valley/logged_VF_data/collar logs_download2/

# step 1 create function to import the data.
This function attaches the collar name to the file
creates a sensiable time column and writes the files out to another directory.
Note this could be changes if you are working on a vitrual machine.

```{r setting up functions to import the data, eval=FALSE, include=FALSE}
read_csv_FUN <- function(file ){
  the_data <- read_csv(file, col_types = cols(value = col_character()))
}


import_function <- function(mydir){
  
  myfiles = list.files(path=mydir, pattern="*.csv", full.names=TRUE)
  #1a########### Get file names
  filenames <- myfiles
  collars <- str_extract(myfiles, "[a-z]+\\d{1,6}")
  
  #1b########## Get length of each csv
  file_lengths <- unlist(lapply(lapply(filenames, read_csv), nrow))
  #file_lengths
  #1c########## Repeat collars using lengths
  file_names <- rep(collars,file_lengths)
  
  #1d######### Create table
  tbl <- lapply(filenames, read_csv_FUN) %>% #this call the function read_csv_FUN I made outside this import_function
    bind_rows()
  #1e######### Combine file_names and tbl
  VF <- cbind(tbl, collar_ID = file_names)
  #glimpse(VF)
  
  ################## Step 2 extra clms for  raw logged data   ##############################################
  VF <- VF %>% 
    separate(collar_ID,into =  c("collar", "date"),  sep = "_", remove = FALSE ) %>% 
    mutate(hms = hms::as.hms(time, tz="GMT"),
           date= date(time),
           month = month(time),
           day = day(time))
  write_csv(VF, path = paste0("W:/VF/Eden_Valley/logged_VF_data/download2_R_output/", "VF_", mydir, ".csv")) 
}


```

# step 2 Use the function to import the data and combine
It would be better if this was a loop.


```{r use function bring n data, eval=FALSE, include=FALSE}
##### Use the function to bring in data for one day that is specified ######
VF_20190607 <- import_function("20190607")
VF_20190606 <- import_function("20190606")
VF_20190605 <- import_function("20190605")
VF_20190604 <- import_function("20190604")
VF_20190603 <- import_function("20190603")
VF_20190602 <- import_function("20190602") #not written r bind 
VF_20190601 <- import_function("20190601")
VF_20190531 <- import_function("20190531")
VF_20190530 <- import_function("20190530")
VF_20190529 <- import_function("20190529")
VF_20190528 <- import_function("20190528")
VF_20190527 <- import_function("20190527")
VF_20190526 <- import_function("20190526")
VF_20190525 <- import_function("20190525")
VF_20190524 <- import_function("20190524")
VF_20190523 <- import_function("20190523")
VF_20190522 <- import_function("20190522")
VF_20190521 <- import_function("20190521")
VF_20190520 <- import_function("20190520")
VF_20190519 <- import_function("20190519")
VF_20190518 <- import_function("20190518")
VF_20190517 <- import_function("20190517")

VF_week1 <- rbind(VF_20190517, VF_20190518, VF_20190519,
                  VF_20190520, VF_20190521, VF_20190522, VF_20190523)
VF_week2 <- rbind(VF_20190524, VF_20190525, VF_20190526,
                  VF_20190527, VF_20190528, VF_20190529, VF_20190530)
VF_week3 <- rbind(VF_20190531, VF_20190601, 
                  VF_20190602, #not written ? not sure why
                  VF_20190603, VF_20190604, VF_20190605, VF_20190606)

write_csv(VF_week1, path = paste0("W:/VF/Eden_Valley/logged_VF_data/download2_R_output/", "VF_week1.csv"))
write_csv(VF_week2, path = paste0("W:/VF/Eden_Valley/logged_VF_data/download2_R_output/", "VF_week2.csv"))
write_csv(VF_week3, path = paste0("W:/VF/Eden_Valley/logged_VF_data/download2_R_output/", "VF_week3.csv"))
glimpse(VF_20190607)

```

# These steps are quite long
A short cut and a way to improve running R would be just to start here with the merged files.
```{r import merged week1 - 3 files}

VF_week1 <- read_csv("//172.20.104.21/OSM_Mel_ces_spatiotemp_scratch/Users/Jackie/VF_week1.csv")
VF_week2 <- read_csv("//172.20.104.21/OSM_Mel_ces_spatiotemp_scratch/Users/Jackie/VF_week2.csv")
VF_week3 <- read_csv("//172.20.104.21/OSM_Mel_ces_spatiotemp_scratch/Users/Jackie/VF_week3.csv")

#Merge this all togther
VF_week1_2_3 <- rbind(VF_week1, VF_week2, VF_week3)
# ensure the column value is a number - double
VF_week1_2_3_InclusionBord <- filter(VF_week1_2_3, event == "InclusionBorder_m") %>%   
  mutate( value = as.double(value))


### Remove the NA ####

VF_week1_2_3_InclusionBord <- VF_week1_2_3_InclusionBord %>% filter(!is.na(lat) | !is.na(lon))

summary(VF_week1_2_3_InclusionBord$lat)
summary(VF_week1_2_3_InclusionBord$lon)
```


#3.Do projections
 

#https://spatialreference.org/ref/epsg/gda94-mga-zone-56/
#epsg projection 28356

```{r project data}

mapCRS <- CRS("+init=epsg:28354")     # 28355 = GDA_1994_MGA_Zone_54
wgs84CRS <- CRS("+init=epsg:4326")   # 4326 WGS 84 - assumed for input lats and longs

####################  convert lat and longs to x and Y     ##########################################
coordinates(VF_week1_2_3_InclusionBord) <- ~ lon + lat
proj4string(VF_week1_2_3_InclusionBord) <- wgs84CRS   # assume input lat and longs are WGS84
#make new object_1
VF_week1_2_3_InclusionBord_1 <- spTransform(VF_week1_2_3_InclusionBord, mapCRS)
#make new df_1
head(VF_week1_2_3_InclusionBord_1,3)
VF_week1_2_3_InclusionBord = as.data.frame(VF_week1_2_3_InclusionBord_1) #this has the new coordinates projected !YES!!
#make new df with point x and point y
VF_week1_2_3_InclusionBord <- mutate(VF_week1_2_3_InclusionBord,POINT_X = lon,  POINT_Y = lat )
head(VF_week1_2_3_InclusionBord)

```


# 4. Now I need to view each day collar to work out when the collars failed.
I have done this using the filter function and then check out the distance from fence.

```{r inspect data to view quaility}
dim(VF_week1_2_3_InclusionBord)
VF_week1_2_3_InclusionBord %>% 
  filter(date == "2019-05-17") %>% 
  ggplot(aes(x = hms, y = value, colour = collar))+
  geom_point()+
  facet_wrap(.~collar)+
  geom_hline(yintercept = 0)+
  theme(axis.text.x=element_text(angle=90,hjust=1),
        legend.position = "none")+
  labs(title= "weekX 2019-xx-xx",
       x= "Time of day",
       y = "Distance (m) from VF")
```

# Time to fix up the data 
The first task with this is to remove the data points that have failed.

```{r remove pre trial values}
#############  Remove Pre trial readings ##############

VF_week1_2_3_InclusionBord <- filter(VF_week1_2_3_InclusionBord, time > as_datetime('2019-05-20 15:00:00', tz="GMT"))

```

Remove week one odd readings.
I am doing this by creating a new data column called chuck.
I assign a value of 1 if I intent to remove it and a value 2 if I want to retain it.
ac209 20/5/2019 
ac207 23/5/2019 07:00-10:00
ac213 23/5/2019 07:00 -10:00



```{r filter out week 1 readings}
################################################################################################################
###################      Filter out rows of data for 20/5/2019 ac209  #################################
VF_week1_2_3_InclusionBord <-  mutate(VF_week1_2_3_InclusionBord,
                                      chuck = case_when(
                                        collar_ID == "ac209" & 
                                          date == as_datetime('2019-05-20') ~ 1,
                                                  TRUE ~ 2))
#check_week1 <- filter(VF_week1_2_3_InclusionBord,
                      #chuck == 1)
#dim(check_week1)

################################################################################################################
###################      Filter out rows of data for 23/5/2019 ac207 and ac213 #################################
################### This creates a clm called chuck for the data to be discarded

################ for ac207 between 08:00 to 09:00 on the 23/5/2019 ####

VF_week1_2_3_InclusionBord <-  mutate(VF_week1_2_3_InclusionBord,
                               chuck = case_when(
                               collar_ID == "ac207" & 
                               between(time, as_datetime('2019-05-23 07:00:00', tz="GMT"),
                               as_datetime('2019-05-23 10:00:00', tz="GMT")) ~1,
                               TRUE ~ chuck))

#check_week1 <- filter(VF_week1_2_3_InclusionBord, chuck == 1)
#dim(check_week1)
################ for ac213 between 08:00 to 09:00 on the 23/5/2019 ####

VF_week1_2_3_InclusionBord <-  mutate(VF_week1_2_3_InclusionBord,
                                      chuck = case_when(
                                        collar_ID == "ac213" & 
                                          between(time, as_datetime('2019-05-23 07:00:00', tz="GMT"),
                                                  as_datetime('2019-05-23 10:00:00', tz="GMT")) ~1,
                                        TRUE ~ chuck))


#check_week1 <- filter(VF_week1_InclusionBord_VF1,chuck == 1)
#summary(check_week1)
#dim(check_week1)

```

Remove week two odd readings.
I am doing this by creating a new data column called chuck.
I assign a value of 1 if I intent to remove it and a value 2 if I want to retain it.
To chuck:
ac219 from 15:00 onwards      on the  24/5/2019
ac219                         on the  25/5/2019 
ac220  before  11:00  on the  on the 25/5/2019 
ad3925 before 20:30 to 22:30  on the  25/5/2019
ad3396 between 1500-23:00     on the  27/5/2019
ad3925 between 19:30-20:30     on the  27/5/2019
ac320 between 11:00-11:30     on the  28/5/2019
ac209 between 11:00-11:30     on the  28/5/2019
ac211 between 11:00-11:30     on the  28/5/2019
ad3396 between 10:00-12:30     on the  28/5/2019
ac211 between whole day     on the  28/5/2019
ad2042 value >3000 on the 28/5/2019
ac333 06:00-14:00 on the 30/5/2019







```{r filter out week 2 readings}
################ for ac219 from 15:00  on the 24/5/2019 and 25/5/2019####

VF_week1_2_3_InclusionBord <-  mutate(VF_week1_2_3_InclusionBord,
                                      chuck = case_when(
                                        collar_ID == "ac219" & 
                                          between(time, as_datetime('2019-05-24 15:00:00', tz="GMT"),
                                               as_datetime('2019-05-24 24:00:00', tz="GMT")) ~1,
                                        TRUE ~ chuck))
VF_week1_2_3_InclusionBord <-  mutate(VF_week1_2_3_InclusionBord,
                                      chuck = case_when(
                                        collar_ID == "ac219" & 
                                          date == as_datetime('2019-05-25') ~ 1,
                                        TRUE ~ chuck))

############### for ac220 before 11:00  on the  25/5/2019####

VF_week1_2_3_InclusionBord <-  mutate(VF_week1_2_3_InclusionBord,
                                  chuck = case_when(
                                    collar_ID == "ac220" & 
                                      time < as_datetime('2019-05-25 11:00:00', tz="GMT") ~ 1,
                                    TRUE ~ chuck))

############### for ad3925 before 20:30 to 21:30  on the  25/5/2019####

VF_week1_2_3_InclusionBord <-  mutate(VF_week1_2_3_InclusionBord,
                                   chuck = case_when(
                                     collar_ID == "ad3925" & 
                                       between(time, as_datetime('2019-05-25 20:30:00', tz="GMT"),
                                               as_datetime('2019-05-25 22:30:00', tz="GMT")) ~1,
                                     TRUE ~ chuck))

############### for ad3396 between 1500-23:00  on the  27/5/2019####

VF_week1_2_3_InclusionBord <-  mutate(VF_week1_2_3_InclusionBord,
                                      chuck = case_when(
                                        collar_ID == "ad3396" & 
                                          between(time, as_datetime('2019-05-27 15:00:00', tz="GMT"),
                                                  as_datetime('2019-05-27 23:00:00', tz="GMT")) ~1,
                                        TRUE ~ chuck))



############### for ad3925 between 1500-23:00  on the  27/5/2019####
#table(VF_week2_InclusionBord1$collar_ID)
VF_week1_2_3_InclusionBord <-  mutate(VF_week1_2_3_InclusionBord,
                                   chuck = case_when(
                                     collar_ID == "ad3925" & 
                                       between(time, as_datetime('2019-05-27 19:30:00', tz="GMT"),
                                               as_datetime('2019-05-27 20:30:00', tz="GMT")) ~1,
                                     TRUE ~ chuck))




############### for ac320 between 10:00-11:45  on the  28/5/2019####

VF_week1_2_3_InclusionBord <-  mutate(VF_week1_2_3_InclusionBord,
                                   chuck = case_when(
                                     collar_ID == "ac320" & 
                                       between(time, as_datetime('2019-05-28 11:00:00', tz="GMT"),
                                               as_datetime('2019-05-28 11:30:00', tz="GMT")) ~1,
                                     TRUE ~ chuck))



############### for ac209 between 10:00-11:45  on the  28/5/2019####

VF_week1_2_3_InclusionBord <-  mutate(VF_week1_2_3_InclusionBord,
                                   chuck = case_when(
                                     collar_ID == "ac209" & 
                                       between(time, as_datetime('2019-05-28 11:00:00', tz="GMT"),
                                               as_datetime('2019-05-28 11:30:00', tz="GMT")) ~1,
                                     TRUE ~ chuck))



############### for ac211 between 10:00-11:45  on the  28/5/2019####

VF_week1_2_3_InclusionBord <-  mutate(VF_week1_2_3_InclusionBord,
                                   chuck = case_when(
                                     collar_ID == "ac211" & 
                                       between(time, as_datetime('2019-05-28 11:00:00', tz="GMT"),
                                               as_datetime('2019-05-28 11:30:00', tz="GMT")) ~1,
                                     TRUE ~ chuck))



############### for ad3396 between 10:00-11:45  on the  28/5/2019####

VF_week1_2_3_InclusionBord <-  mutate(VF_week1_2_3_InclusionBord,
                                   chuck = case_when(
                                     collar_ID == "ad3396" & 
                                       between(time, as_datetime('2019-05-28 10:00:00', tz="GMT"),
                                               as_datetime('2019-05-28 12:30:00', tz="GMT")) ~1,
                                     TRUE ~ chuck))


############### for ac211 between 10:00-11:45  on the  28/5/2019####
VF_week1_2_3_InclusionBord <-  mutate(VF_week1_2_3_InclusionBord,
                                      chuck = case_when(
                                        collar_ID == "ac211" & 
                                          date == as_datetime('2019-05-28') ~ 1,
                                        TRUE ~ chuck))


############### for ad2042 for values greater than 3000meter from VF  on the  28/5/2019####
VF_week1_2_3_InclusionBord <-  mutate(VF_week1_2_3_InclusionBord,
                                   chuck = case_when(
                                     collar_ID == "ad2042" & 
                                       value > 3000  ~ 1,
                                     TRUE ~ chuck))


############### for ac333   on the 06:00 - 14:00 30/5/2019####
#table(VF_week2_InclusionBord1$collar_ID)
VF_week1_2_3_InclusionBord <-  mutate(VF_week1_2_3_InclusionBord,
                                   chuck = case_when(
                                     collar_ID == "ac333" & 
                                       between(time, as_datetime('2019-05-30 06:00:00', tz="GMT"),
                                               as_datetime('2019-05-30 14:00:00', tz="GMT")) ~1,
                                     TRUE ~ chuck))

check_week2 <- filter(VF_week1_2_3_InclusionBord,
                      chuck == 1)
dim(check_week2)



```

Now remove the rows we dont want
```{r filtering out the rows}

VF_week1_2_3_InclusionBord <- filter(VF_week1_2_3_InclusionBord, chuck == 2)

VF_week1_2_3_InclusionBord <- mutate(VF_week1_2_3_InclusionBord,
                                    hour= hour(time))

```

Check out what we have done??
```{r}
VF_week1_2_3_InclusionBord %>% 
  filter(date == "2019-05-20") %>% 
  ggplot(aes(x = POINT_X, y = POINT_Y, colour = hour))+
  geom_point()+
  facet_wrap(.~collar)+
  labs(title= "week1 2019-05-xx",
       x= "POINT_X",
       y = "POINT_Y")+
  theme(axis.text.x=element_text(angle=90,hjust=1),
        legend.position = "none")
```


Still got some problem points...

I can use the clip(from sf) function but I need to convert it into a df and I am not sure how to get it back
1) turn data into spatial data
```{r use clip}

### turn my data into 'shapefile' format - not sure i need this step 

mapCRS <- CRS("+init=epsg:28354")     # 28355 = GDA_1994_MGA_Zone_54

VF_week1_2_3_InclusionBord_spatial <- st_as_sf(VF_week1_2_3_InclusionBord, coords = c("POINT_X", "POINT_Y"), crs = mapCRS)
glimpse(VF_week1_2_3_InclusionBord_spatial)




```



2)Bring in the paddock and VF boundaries and plot it

```{r}
###bring in the polygon file
eden_valley <- st_read(
  "W:/VF/Eden_Valley/VF_Boundary/EdenValley_site1GDA_a.shp")

fence1 <- st_read(
  "W:/VF/Eden_Valley/VF_Boundary/Fence1.shx")
fence2 <- st_read(
  "W:/VF/Eden_Valley/VF_Boundary/Fence2.shx")
fence3 <- st_read(
  "W:/VF/Eden_Valley/VF_Boundary/Fence3.shx")
fence4 <- st_read(
  "W:/VF/Eden_Valley/VF_Boundary/Fence4a.shx")
fence5 <- st_read(
  "W:/VF/Eden_Valley/VF_Boundary/Fence_hotwire_rd_side_clipped.shx")


ggplot()+
  geom_sf(data = eden_valley, size = 2 , colour = "black", fill=NA)+
  geom_sf(data = fence1, size = 1 , colour = "black")+
  geom_sf(data = fence2, size = 1 , colour = "grey")+
  geom_sf(data = fence3, size = 1 , colour = "blue")+
  geom_sf(data = fence4, size = 1 , colour = "red")+
  geom_sf(data = fence5, size = 1 , colour = "pink")+
  coord_sf()

```

  #) clip function

```{r clip }
VF_week1_2_3_InclusionBord_clip <- st_intersection(VF_week1_2_3_InclusionBord_spatial, eden_valley)
head(VF_week1_2_3_InclusionBord_clip)
```


Plot

```{r might be too big?}

##subset the data so I can work out what I am doing 
VF_week1_2_3_InclusionBord_clip_subset <- filter(VF_week1_2_3_InclusionBord_clip, 
                                          between(time, as_datetime('2019-05-28 08:00:00', tz="GMT"),
                                                        as_datetime('2019-05-28 09:30:00', tz="GMT"))) 




ggplot()+
    geom_sf(data = eden_valley, size = 2 , colour = "black", fill=NA)+
    geom_sf(data = fence1, size = 1 , colour = "black")+
    geom_sf(data = fence2, size = 1 , colour = "grey")+
    geom_sf(data = fence3, size = 1 , colour = "blue")+
    geom_sf(data = fence4, size = 1 , colour = "red")+
    geom_sf(data = fence5, size = 1 , colour = "pink")+
    geom_sf(data = VF_week1_2_3_InclusionBord_clip_subset, size = 1 , colour = "black")
  coord_sf()
```

Not sure how to get my spatial data back to df
also need to create a new colum for the animal number and work out when to change it over

Mess about getting my spatial data back into data frame
```{r back to df}

head(VF_week1_2_3_InclusionBord_clip_subset ,3)

VF_week1_2_3_InclusionBord_clip_subset = as.data.frame(VF_week1_2_3_InclusionBord_clip_subset) 
#make new df with point x and point y
VF_week1_2_3_InclusionBord_clip_subset <- mutate(VF_week1_2_3_InclusionBord_clip_subset,POINT_X = lon,  POINT_Y = lat )

head(VF_week1_2_3_InclusionBord_clip_subset ,3)


##try on real data
head(VF_week1_2_3_InclusionBord_clip ,3)
df_VF_week1_2_3_InclusionBord_clip <- data.frame(VF_week1_2_3_InclusionBord_clip)
df_VF_week1_2_3_InclusionBord_clip <- mutate(df_VF_week1_2_3_InclusionBord_clip,
                                             POINT_X = lon,  POINT_Y = lat )

```


Make an annimation

```{r annimation}
###can split this up a bit and add in larger data set
##subset data to messing about

head(VF_week1_2_3_InclusionBord_clip_subset ,3)

animmation_data <- VF_week1_2_3_InclusionBord_clip_subset %>% 
  filter(date == "2019-05-28") 
#write.csv(animmation_data, "animmation_data.csv")

head(animmation_data,3)

p4a <- ggplot() +
  geom_sf(data = eden_valley, color = "black", fill = NA) +
  geom_sf(data = fence1, color = "grey") +
  geom_sf(data = fence2, color = "grey") +
  geom_sf(data = fence3, color = "grey") +
  geom_sf(data = fence4, color = "grey") +
  geom_sf(data = fence5, color = "grey") +
  geom_point(data = animmation_data, aes(POINT_X, POINT_Y,colour = collar), inherit.aes = FALSE) +
  #facet_wrap(.~collar)+
  theme_bw()+
  theme(legend.position = "none",
        axis.text.x=element_text(angle=90,hjust=1))

  
p4a  
  #as_datetime('2019-05-28 08:00:00', tz="GMT"
p4b <- p4a +
  labs( title = 'Date: {format(frame_time, "%b %e")}',
        subtitle = 'Hour: {format(as_datetime(frame_time, "%H"), tz="GMT")}',
       caption = "Frame {frame} of {nframes} ({progress * 100}%)") +
  transition_time(time) +
  shadow_wake(0.3)

animate(p4b, duration = 30) 
```


